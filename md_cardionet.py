# -*- coding: utf-8 -*-
"""MD_CardioNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1votd9jcAtUPmKa9GeHfjlJFjLvFk59EX
"""

#Importing necessary Libraries 

import tensorflow as tf
import numpy as np
import keras
import random
from keras import backend as K
from keras.models import Sequential
from keras.models import Model
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
from keras.layers import Input, Embedding, LSTM, Dense, Conv2D, Conv1D, AveragePooling1D, MaxPool2D,GlobalAveragePooling2D,MaxPool1D,GlobalAveragePooling1D,BatchNormalization,Flatten,Bidirectional,Conv3D,MaxPool3D,GlobalAveragePooling3D
from keras.optimizers import Adam
from keras.layers import concatenate
from keras.regularizers import l2
from keras.callbacks import ModelCheckpoint
from keras import optimizers, callbacks
from tensorflow.keras.utils import to_categorical 
import random as rn

#Setting up the random SEED

import os
os.environ['PYTHONHASHSEED']='0'
np.random.seed(13817)
rn.seed(12513)
tf.random.set_seed(8130)

def line1(x):
  # received shape: (256*64)

  conv_1=Conv1D(128,14,activation='relu',padding='same')(x)
  pool_1=MaxPool1D(2)(conv_1)
  reshaped_1=tf.reshape(pool_1,[tf.shape(x)[0],128,128,1])
  maxpool_1=MaxPool2D(8)(reshaped_1)
  extra_line_1=line3D_1(maxpool_1)



  #2D net
  block1_conv1 = Conv2D(32,(14,14),activation='relu',padding='same')(reshaped_1)
  block1_pool  = MaxPool2D(2,2)(block1_conv1)

  block2_conv1 = Conv2D(64,(7,7),activation='relu',padding='same')(block1_pool)
  block2_pool  = MaxPool2D(2,2)(block2_conv1)
  

  global_pool= GlobalAveragePooling2D()(block2_pool)

  fully_connected_1= Dense(512,activation='relu')(global_pool)

  return fully_connected_1,extra_line_1

x=np.ones((20,256,64))
net1,net2=line1(x)
net2.shape

def line2(x):
  # received shape: (64*128)

  conv_1=Conv1D(64,14,activation='relu',padding='same')(x)
  reshaped_1=tf.reshape(conv_1,[tf.shape(x)[0],64,64,1])
  block1_pool=MaxPool2D(4)(reshaped_1)
  print(block1_pool.shape)
  extra_line_1=line3D_1(block1_pool)
  print(extra_line_1.shape)




  #2D net
  block1_conv1 = Conv2D(32,(7,7),activation='relu',padding='same')(block1_pool)
  block1_pool  = MaxPool2D(2,2)(block1_conv1)

  block2_conv1 = Conv2D(64,(5,5),activation='relu',padding='same')(block1_pool)
  block2_pool  = MaxPool2D(2,2)(block2_conv1)
  global_pool= GlobalAveragePooling2D()(block2_pool)

  fully_connected_1= Dense(512,activation='relu')(global_pool)

  return fully_connected_1,extra_line_1
  #return out

x=np.ones((20,64,128))
net=line2(x)
net.shape

def line3(x):
  # received shape: (16*256)

  conv_1=Conv1D(16,21,activation='relu',padding='same')(x)
  reshaped_1=tf.reshape(conv_1,[tf.shape(x)[0],16,16,1])
  extra_line_1=line3D_1(reshaped_1)

 





  #2D net
  block1_conv1 = Conv2D(32,(7,7),activation='relu',padding='same')(reshaped_1)
  block1_pool  = MaxPool2D(2,2)(block1_conv1)

  block2_conv1 = Conv2D(64,(5,5),activation='relu',padding='same')(block1_pool)
  block2_pool  = MaxPool2D(2,2)(block2_conv1)



  global_pool= GlobalAveragePooling2D()(block2_pool)

  fully_connected_1= Dense(512,activation='relu')(global_pool)

  return fully_connected_1,extra_line_1

def line3D_1(x):  

  # For the 3D convolution
  conv_1=Conv2D(16,(3,3),activation='relu',padding='same')(x)
  
  reshaped_2=tf.reshape(conv_1,[tf.shape(x)[0],16,16,16,1])
  conv_1=Conv3D(32,(4,4,4),activation='relu',padding='same')(reshaped_2)
  conv_1=MaxPool3D(2)(conv_1)
  conv_1=Conv3D(64,(2,2,2),activation='relu',padding='same')(conv_1)
  conv_1=MaxPool3D(2)(conv_1)
  conv_1=Conv3D(128,(2,2,2),activation='relu',padding='same')(conv_1)
  conv_1=MaxPool3D(2)(conv_1)
  conv_1=GlobalAveragePooling3D()(conv_1)
  out=Dense(512,activation='relu')(conv_1)
  return out

x=np.ones((20,16,256))
net=line3(x)
net.shape

def line4(x):

  # The LSTM network after the 1D Sub-unit
  #received shape: (16*256)
  conv_1=Conv1D(512,14,activation='relu',padding='same')(x)
  conv_1=MaxPool1D(4)(conv_1)
  conv_1=BatchNormalization()(conv_1)
  lstm_1=LSTM(256,return_sequences=False)(conv_1)
  flat=Flatten()(lstm_1)
  out=Dense(512,activation='relu')(flat)


  return out

def line5(x):

  # The parallel sequential LSTM feature optimizer
  x=BatchNormalization()(x)
  lstm1=Bidirectional(LSTM(128,return_sequences=False))(x)
  out=Dense(512,activation='relu')(lstm1)

  return out

def getModel():
  x=Input(shape=(1024,12),name='x')
  f_lstm=line5(x)

  # converting to (256*64):

  conv_1=Conv1D(64,21,activation='relu',padding='same',strides=1)(x)
  conv_1=MaxPool1D(4)(conv_1) # (256*64)

  f1,f1_3d=line1(conv_1)

  # converting to (64*128):

  conv_1=Conv1D(128,21,activation='relu',padding='same',strides=1)(conv_1)
  conv_1=MaxPool1D(4)(conv_1) # (64*128)

  f2,f2_3d=line2(conv_1)

  # converting to (16*256):

  conv_1=Conv1D(128,21,activation='relu',padding='same',strides=1)(conv_1)
  conv_1=MaxPool1D(4)(conv_1) # (16*256) 
  print(conv_1.shape)

  f3,f3_3d=line3(conv_1)


  # converting to (16*256): # same input for line4 function 

  f4=line4(conv_1)

  







  # concate the 4 dense layer channelwise

  f1=tf.reshape(f1,[tf.shape(x)[0],512,1])
  f2=tf.reshape(f2,[tf.shape(x)[0],512,1])
  f3=tf.reshape(f3,[tf.shape(x)[0],512,1])
  f4=tf.reshape(f4,[tf.shape(x)[0],512,1])
  f5=tf.reshape(f_lstm,[tf.shape(x)[0],512,1])
  f6=tf.reshape(f1_3d,[tf.shape(x)[0],512,1])
  f7=tf.reshape(f2_3d,[tf.shape(x)[0],512,1])
  f8=tf.reshape(f3_3d,[tf.shape(x)[0],512,1])

  concat= tf.keras.layers.Concatenate()([f1,f2,f3,f4,f5,f6,f7,f8])
  concat=tf.reduce_mean(concat,axis=2)


  dense_1=Dense(128,activation='relu')(concat)
  dense_2=Dense(32,activation='relu')(dense_1)
  out=Dense(6,activation='softmax')(dense_2)
  

  #print(out.shape)
  model=Model(inputs=x,outputs=out)








  return model

model=getModel()

model.summary()

sgd = optimizers.Adam(lr=0.001, clipnorm=0.9)
model.compile(optimizer='adam',
              loss='categorical_crossentropy',metrics=['accuracy'])

# Importing the Datasets 
import pickle

with open('/content/drive/MyDrive/ecg research/dataset/output_nature_balanced','rb') as f: y1 = pickle.load(f)
with open('/content/drive/MyDrive/ecg research/dataset/input_nature_balanced','rb') as f1: x1 = pickle.load(f1)

x=x1
y=y1
x=x1
y=y1
x=np.nan_to_num(x)
x.shape

y=to_categorical(y)
y.shape

#Train and Test Split

xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6=x[:1900],x[2000:3900],x[4000:5900],x[6000:7900],x[8000:9900],x[10000:11900]
xtest1,xtest2,xtest3,xtest4,xtest5,xtest6=x[1900:2000],x[3900:4000],x[5900:6000],x[7900:8000],x[9900:10000],x[11900:12000]
x_train=np.concatenate([xtrain1,xtrain2,xtrain3,xtrain4,xtrain5,xtrain6],axis=0)
x_test=np.concatenate([xtest1,xtest2,xtest3,xtest4,xtest5,xtest6],axis=0)

ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6=y[:1900],y[2000:3900],y[4000:5900],y[6000:7900],y[8000:9900],y[10000:11900]
ytest1,ytest2,ytest3,ytest4,ytest5,ytest6=y[1900:2000],y[3900:4000],y[5900:6000],y[7900:8000],y[9900:10000],y[11900:12000]
y_train=np.concatenate([ytrain1,ytrain2,ytrain3,ytrain4,ytrain5,ytrain6],axis=0)
y_test=np.concatenate([ytest1,ytest2,ytest3,ytest4,ytest5,ytest6],axis=0)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

def standardize(train, test):


    mean = np.mean(train, axis=0)
    std = np.std(train, axis=0)+0.000001

    X_train = (train - mean) / std
    X_test = (test - mean) /std
    return X_train, X_test

x_train,x_test=standardize(x_train,x_test)

his2=his

his=model.fit(x_train,y_train,epochs=1,verbose=2,batch_size=64)

Y_pred=model.predict(x_test)
y_test=np.argmax(y_test,axis=1)
y_pred=np.argmax(Y_pred,axis=1)

# Printing the confusion matrix

from sklearn.metrics import confusion_matrix
cf_matrix = confusion_matrix(y_test, y_pred)
import seaborn as sns

sns.heatmap(cf_matrix, annot=True)

from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(y_test, y_pred, average='macro')

#Saving Model to Disk

import json
model_json = model.to_json()
with open("/content/drive/MyDrive/ecg research/final_historty/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/content/drive/MyDrive/ecg research/final_historty/model.h5")
print("Saved model to disk")

#Loading model from Disk

from keras.models import model_from_json
json_file = open('/content/drive/MyDrive/ecg research/final_historty/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/content/drive/MyDrive/ecg research/final_historty/model.h5")
print("Loaded model from disk")

Y_pred=loaded_model.predict(x_test)
y_test=np.argmax(y_test,axis=1)
y_pred=np.argmax(Y_pred,axis=1)

from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(y_test, y_pred, average='macro')

from sklearn.metrics import confusion_matrix
cf_matrix = confusion_matrix(y_test, y_pred)
import seaborn as sns

f=sns.heatmap(cf_matrix, annot=True)
fig=f.get_figure()
fig.savefig("hi.png",dpi=300)

#calculating the Inference Time
import datetime
a = datetime.datetime.now()

res=loaded_model.predict(x_test[10:11,:,:])
b = datetime.datetime.now()

print((b-a).total_seconds())

